{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e85ff09",
   "metadata": {},
   "source": [
    "## Create Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68844528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization, LeakyReLU, GlobalAveragePooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import utils_datasets as utils\n",
    "import importlib\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import datetime\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import tensorflow as tf\n",
    "import os\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cef7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "\n",
    "\n",
    "# Input shape: 64x60 grayscale image\n",
    "inputs = layers.Input(shape=(114, 120, 3))\n",
    "\n",
    "x = layers.Conv2D(32, (5,4), padding='same', activation='relu')(inputs)\n",
    "x = layers.Conv2D(32, (5,4), padding='same', activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.MaxPooling2D(pool_size=(2,2))(x)\n",
    "\n",
    "x = layers.Conv2D(64, (4,3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.LeakyReLU(alpha=0.1)(x)\n",
    "x = layers.Conv2D(64, (4,3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.LeakyReLU(alpha=0.1)(x)\n",
    "x = layers.MaxPooling2D(pool_size=(2,2))(x)\n",
    "\n",
    "x = layers.Conv2D(128, (4,3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.LeakyReLU(alpha=0.1)(x)\n",
    "x = layers.Conv2D(128, (4,3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.LeakyReLU(alpha=0.1)(x)\n",
    "x = layers.MaxPooling2D(pool_size=(2,2))(x)\n",
    "\n",
    "x = layers.Conv2D(256, (4,3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.LeakyReLU(alpha=0.1)(x)\n",
    "x = layers.Conv2D(256, (4,3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.LeakyReLU(alpha=0.1)(x)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "x = layers.Dense(256, kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.LeakyReLU(alpha=0.1)(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "outputs = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "# Compile with binary crossentropy\n",
    "model.compile(optimizer=Adam(learning_rate=0.0005),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Show architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b24b25c",
   "metadata": {},
   "source": [
    "# Code chunk to apply the model on one image of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338f50ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "img_path = ''  # <-- update this to your image path\n",
    "\n",
    "# Load the image\n",
    "img = image.load_img(img_path, color_mode='grayscale', target_size=(28, 28))\n",
    "img_array = image.img_to_array(img)\n",
    "model = Sequential()\n",
    "\n",
    "model = model.add(Conv2D(64, kernel_size=(5, 3), padding='same', input_shape=(64, 60, 1)))\n",
    "\n",
    "\n",
    "outout = model.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8137e1",
   "metadata": {},
   "source": [
    "## Fetch dataframes\n",
    "- Get the different datasets used to construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0023a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "#use the datasets that doesn t conaint all the datas (that has been droped in the preceding notebook.)\n",
    "path_dataset = \"\"\n",
    "train_df, val_df, test_df = utils.split_global_dataset(path_dataset)\n",
    "train_ds = utils.dataframe_to_dataset_color(train_df).shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = utils.dataframe_to_dataset_color(val_df).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = utils.dataframe_to_dataset_color(test_df).batch(32)\n",
    "\n",
    "class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_df['y']),\n",
    "    y=train_df['y']\n",
    ")\n",
    "class_weights = {0: class_weights_array[0], 1: class_weights_array[1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d66364",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78074560",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "callbacks = [\n",
    "    TensorBoard(log_dir=log_dir, histogram_freq=1),\n",
    "    #EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint(\"best_model.keras\", save_best_only=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=50,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[callbacks],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d366fb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model_path = \"\"\n",
    "model = load_model(model_path)\n",
    "\n",
    "# loss, acc = model.evaluate(test_ds)\n",
    "# print(f\"Global Test Loss: {loss:.4f}, Global Test Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5cfd1c",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e890e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Storage\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "correct_class_1 = []  # (image, confidence)\n",
    "correct_class_0 = []\n",
    "\n",
    "incorrect_class_1 = []  # (image, confidence)\n",
    "incorrect_class_0 = []\n",
    "\n",
    "\n",
    "#incorrect_class_1 = True class was 1, but model predicted wrong\n",
    "for X_batch, y_batch in test_ds:\n",
    "    preds = model.predict(X_batch, verbose=0)\n",
    "    preds_class = np.argmax(preds, axis=1)  # Predicted classes\n",
    "    true_class = y_batch.numpy().astype(int)  # True labels\n",
    "\n",
    "    y_true.extend(true_class)\n",
    "    y_pred.extend(preds_class)\n",
    "\n",
    "    # Go through batch\n",
    "    for i in range(len(preds_class)):\n",
    "        if preds_class[i] == true_class[i]:  # Correct prediction\n",
    "            if preds_class[i] == 1:\n",
    "                correct_class_1.append([X_batch[i], preds[i][1]])  # (image, confidence for class 1)\n",
    "            elif preds_class[i] == 0:\n",
    "                correct_class_0.append([X_batch[i], preds[i][0]])  # (image, confidence for class 0)\n",
    "        else:  # Incorrect prediction\n",
    "            if true_class[i] == 1:\n",
    "                incorrect_class_1.append([X_batch[i], preds[i][1]])  # Actually class 1, but predicted wrong\n",
    "            elif true_class[i] == 0:\n",
    "                incorrect_class_0.append([X_batch[i], preds[i][0]])  # Actually class 0, but predicted wrong\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Class 0\", \"Class 1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176d606a",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af362c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Class 0\", \"Class 1\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix on Test Set\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3c4c33",
   "metadata": {},
   "source": [
    "## Explore features of importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5072a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], \n",
    "        [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(predictions[0])\n",
    "        class_channel = predictions[:, pred_index]\n",
    "\n",
    "    grads = tape.gradient(class_channel, conv_outputs)\n",
    "\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "def make_gradcam_heatmap_from_tensor(img_tensor, model, last_conv_layer_name, pred_index=None):\n",
    "    # img_tensor must be shape (1, height, width, channels)\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], \n",
    "        [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_tensor)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(predictions[0])\n",
    "        class_channel = predictions[:, pred_index]\n",
    "\n",
    "    grads = tape.gradient(class_channel, conv_outputs)\n",
    "\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy(), predictions.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dd1758",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "\n",
    "\n",
    "# # Load your model\n",
    "model_path = \"\"\n",
    "model = load_model(model_path)\n",
    "\n",
    "\n",
    "\n",
    "img_tensor, _ = correct_class_1[6]  # Take the tensor only\n",
    "\n",
    "img_tensor_expanded = tf.expand_dims(img_tensor, axis=0)  # Model expects (1,114,120,3)\n",
    "\n",
    "heatmap, predictions = make_gradcam_heatmap_from_tensor(img_tensor_expanded, model, last_conv_layer_name='conv2d_7')\n",
    "\n",
    "\n",
    "\n",
    "predicted_class = np.argmax(predictions[0])\n",
    "predicted_confidence = np.max(predictions[0])\n",
    "\n",
    "utils.plot_gradcam_side_by_side_from_tensor(img_tensor, heatmap, predicted_class, predicted_confidence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409704ec",
   "metadata": {},
   "source": [
    "# Select the stocks on which we ll test our hedging process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c32c130",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset path (the one with all the datas)\n",
    "base_path = \"\"\n",
    "tickers = os.listdir(base_path)\n",
    "model = load_model('/Users/edouardblanc/Desktop/aze.keras')\n",
    "df = utils.load_all_ticker_data(base_path)\n",
    "\n",
    "for ticker in tickers:\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    df_ticker = df[df['ticker'] == ticker]\n",
    "    df_ticker= utils.dataframe_to_dataset_color(df_ticker).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "    for X_batch, y_batch in df_ticker:\n",
    "        preds = model.predict(X_batch, verbose=0)\n",
    "        preds_class = np.argmax(preds, axis=1)  # Predicted classes\n",
    "        true_class = y_batch.numpy().astype(int)  # True labels\n",
    "\n",
    "        y_true.extend(true_class)\n",
    "        y_pred.extend(preds_class)\n",
    "\n",
    "\n",
    "    print(f'Ticker report : {ticker}')\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"Class 0\", \"Class 1\"]))\n",
    "\n",
    "\n",
    "    \n",
    "#right ticker ups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e9fb18",
   "metadata": {},
   "source": [
    "# Visualisation of the choosen stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c294e86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import yfinance as yf\n",
    "\n",
    "ticker_selected = 'UPS'\n",
    "df_ticker = utils.download_historical_prices(ticker_selected)\n",
    "start_index = df_ticker['MA20'].first_valid_index()\n",
    "\n",
    "\n",
    "for first_day in range(start_index, df_ticker.shape[0] - 20-1):\n",
    "    df_extract = df_ticker.iloc[first_day:first_day + 20, :]\n",
    "    image = utils.generate_ohlc_image_bis(df_extract, include_volume=True, include_ma=True, n_days=20, target_size=None)\n",
    "    y =  int(df_extract.iloc[-1]['Open'] * (1 + 0.007) <  df_ticker.iloc[first_day + 20]['Close'])\n",
    "    break\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "\n",
    "df_plot =  yf.download(ticker_selected, start=\"2000-01-01\")\n",
    "df_plot.columns = df_plot.columns.get_level_values(0)\n",
    "df_plot = df_plot[['Open', 'Close']].copy()\n",
    "df_plot.columns.name = None \n",
    "df_plot = df_plot.iloc[2206:3000]\n",
    "#2206, 2245\n",
    "prices = []\n",
    "x_vals = []\n",
    "\n",
    "open_prices = []\n",
    "close_prices = []\n",
    "open_x = []\n",
    "close_x = []\n",
    "\n",
    "# Build the curves and markers\n",
    "for i, row in df_plot.iterrows():\n",
    "\n",
    "    # Line: alternating Open/Close to form the zigzag price path\n",
    "    prices.append(row['Open'])\n",
    "    prices.append(row['Close'])\n",
    "    x_vals.append(row.name)\n",
    "    x_vals.append(row.name)\n",
    "\n",
    "    # Markers\n",
    "    open_prices.append(row['Open'])\n",
    "    close_prices.append(row['Close'])\n",
    "    open_x.append(row.name)\n",
    "    close_x.append(row.name)\n",
    "\n",
    "# Main price line\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x_vals,\n",
    "    y=prices,\n",
    "    mode='lines',\n",
    "    name='Price Line',\n",
    "    line=dict(color='lightblue'),\n",
    "    hovertemplate='Index: %{x}<br>Price: %{y:.2f}<extra></extra>'\n",
    "))\n",
    "\n",
    "# Open price markers (small red dots)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=open_x,\n",
    "    y=open_prices,\n",
    "    mode='markers',\n",
    "    name='Open',\n",
    "    marker=dict(color='green', size=6, symbol='circle'),\n",
    "    hovertemplate='Open: %{y:.2f}<extra></extra>'\n",
    "))\n",
    "\n",
    "# Close price markers (small green dots)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=close_x,\n",
    "    y=close_prices,\n",
    "    mode='markers',\n",
    "    name='Close',\n",
    "    marker=dict(color='red', size=6, symbol='circle'),\n",
    "    hovertemplate='Close: %{y:.2f}<extra></extra>'\n",
    "))\n",
    "\n",
    "# Add subtle closing price overlay line (on top, thin, neutral color)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_plot.index,\n",
    "    y=df_plot['Close'],\n",
    "    mode='lines',\n",
    "    name='Close Trace',\n",
    "    line=dict(color='gray', width=1, dash='dot'),  # Thin gray dashed line\n",
    "    hoverinfo='skip',  # Hide hover to avoid clutter\n",
    "    showlegend=False   # Hide from legend for minimalism\n",
    "))\n",
    "\n",
    "# Layout settings\n",
    "fig.update_layout(\n",
    "    title=\"ðŸ“Š UPS Price Evolution (Open vs Close)\",\n",
    "    xaxis_title=\"Time Index\",\n",
    "    yaxis_title=\"Price\",\n",
    "    template=\"plotly_dark\",\n",
    "    height=500,\n",
    "    width=1200,\n",
    "    font=dict(size=14),\n",
    "    margin=dict(l=50, r=50, t=60, b=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5285f3b",
   "metadata": {},
   "source": [
    "## Locate the window that maximizes the event our model is trained to detect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746a1a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ticker['event'] = df_ticker['Close'] > 1.007 * df_ticker['Open']\n",
    "\n",
    "# Step 2: Rolling 40-day sum of True events\n",
    "df_ticker['event_count'] = df_ticker['event'].rolling(window=252).sum()\n",
    "\n",
    "# Step 3: Find the index with max event count\n",
    "max_idx = df_ticker['event_count'].idxmax()\n",
    "start_idx = max_idx - 39\n",
    "end_idx = max_idx\n",
    "print(start_idx, end_idx)\n",
    "df_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46031a76",
   "metadata": {},
   "source": [
    "## Get a proxy for the risk free rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb048fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "risk_free =  yf.download(\"^IRX\", start=\"2000-01-01\")\n",
    "risk_free.columns = risk_free.columns.get_level_values(0)\n",
    "risk_free = risk_free[['Open', 'Close']].copy()\n",
    "risk_free.columns.name = None  # remove 'Price' label above columns\n",
    "risk_free"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6de892",
   "metadata": {},
   "source": [
    "## Get a proxy for dividends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efb58ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_ticker = yf.Ticker(\"UPS\")\n",
    "dividend = stock_ticker.info.get('dividendYield', 0.0)\n",
    "dividend"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "these_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
